# ğŸ“ main_program.py
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import NoSuchElementException
from bs4 import BeautifulSoup
import pandas as pd
import time
import datetime
import gspread
import os
from oauth2client.service_account import ServiceAccountCredentials


def connect_to_google_sheet(sheet_name):
    scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
    creds = ServiceAccountCredentials.from_json_keyfile_name("your-credentials.json", scope)
    client = gspread.authorize(creds)
    try:
        sheet = client.open(sheet_name)
    except gspread.SpreadsheetNotFound:
        sheet = client.create(sheet_name)
    return sheet


def crawl_etf_data():
    # âœ… í˜„ì¬ ì‚¬ìš©ì í™ˆ ë””ë ‰í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
    user_home = os.path.expanduser("~")
    chrome_options = Options()
    chrome_options.add_argument("--start-maximized")
    #service = Service(r"C:\Users\aceof\Documents\chromedriver-win64\chromedriver.exe")  # ìˆ˜ì • í•„ìš”
    chromedriver_path = os.path.join(user_home, "Documents", "chromedriver-win64", "chromedriver.exe")
    service = Service(chromedriver_path)

    driver = webdriver.Chrome(service=service, options=chrome_options)
    driver.get("https://search-etf.com/month_dividend.php")
    time.sleep(2)

    while True:
        try:
            more_button = driver.find_element(
                By.CSS_SELECTOR,
                "div#load-more-desktop button.btn.btn-primary.load-more-btn"
            )
            if not more_button.is_displayed():
                break
            driver.execute_script("arguments[0].scrollIntoView(true);", more_button)
            time.sleep(0.5)
            driver.execute_script("arguments[0].click();", more_button)
            print("âœ… 'ë”ë³´ê¸°' í´ë¦­")
            time.sleep(1.5)
        except NoSuchElementException:
            print("â›” ë”ë³´ê¸° ë²„íŠ¼ ì—†ìŒ â†’ ì¢…ë£Œ")
            break

    html = driver.page_source
    soup = BeautifulSoup(html, "lxml")
    table = soup.select_one("div#desktop-table table")

    df = None
    if table:
        df = pd.read_html(str(table))[0]

        for col_index in [3, 5]:
            df.iloc[:, col_index] = (
                df.iloc[:, col_index].astype(str)
                .str.replace("ì›", "", regex=False)
                .str.replace(",", "", regex=False)
                .str.strip()
            )
            df.iloc[:, col_index] = pd.to_numeric(df.iloc[:, col_index], errors='coerce')

        df.iloc[:, 6] = (
            df.iloc[:, 6].astype(str)
            .str.replace("%", "", regex=False)
            .str.replace(",", "", regex=False)
            .str.strip()
        )
        df.iloc[:, 6] = pd.to_numeric(df.iloc[:, 6], errors='coerce')

        df = df.sort_values(by=df.columns[6], ascending=False).reset_index(drop=True)
        rank_col_name = df.columns[7] if len(df.columns) > 7 else "ìˆœìœ„"
        df.insert(7, rank_col_name, df.index + 1)
    else:
        print("âŒ í…Œì´ë¸”ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    driver.quit()
    return df


def upload_to_google_sheets(df, sheet_name="ì›”ë°°ë‹¹ETF_ëª©ë¡"):
    if df is None or df.empty:
        print("âš ï¸ ì €ì¥í•  ë°ì´í„°í”„ë ˆì„ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return

    df = df.fillna("").astype(str)
    today = datetime.datetime.now().strftime("%Y-%m-%d")
    sheet = connect_to_google_sheet(sheet_name)

    try:
        worksheet = sheet.worksheet(today)
        sheet.del_worksheet(worksheet)
    except gspread.exceptions.WorksheetNotFound:
        pass

    worksheet = sheet.add_worksheet(title=today, rows=str(len(df) + 10), cols=str(len(df.columns) + 5))

    try:
        data = [df.columns.values.tolist()] + df.values.tolist()
        worksheet.update(data)
        print(f"âœ… Google Sheets ì €ì¥ ì™„ë£Œ: [{sheet_name}] â†’ ì‹œíŠ¸ '{today}'")
    except Exception as e:
        print(f"âŒ Google Sheets ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")


def main():
    df = crawl_etf_data()
    upload_to_google_sheets(df)


if __name__ == "__main__":
    main()
